{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBEgb8gF_RRB"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Login using e.g. `huggingface-cli login` to access this dataset\n",
        "ds = load_dataset(\"nielsr/funsd\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers datasets torch pillow pytesseract\n"
      ],
      "metadata": {
        "id": "WaG3UeFj_a59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"nielsr/funsd\")\n",
        "\n",
        "train_dataset = dataset[\"train\"]\n",
        "test_dataset = dataset[\"test\"]\n",
        "\n",
        "print(train_dataset[0])\n"
      ],
      "metadata": {
        "id": "raBapPuDAEKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LABELS = [\n",
        "    \"O\",\n",
        "    \"B-QUESTION\", \"I-QUESTION\",\n",
        "    \"B-ANSWER\", \"I-ANSWER\",\n",
        "    \"B-HEADER\", \"I-HEADER\"\n",
        "]\n",
        "\n",
        "label2id = {label: i for i, label in enumerate(LABELS)}\n",
        "id2label = {i: label for label, i in label2id.items()}\n"
      ],
      "metadata": {
        "id": "hNMDsHkDAIm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LayoutLMv3Processor\n",
        "import torch # Import torch for .squeeze()\n",
        "\n",
        "processor = LayoutLMv3Processor.from_pretrained(\n",
        "    \"microsoft/layoutlmv3-base\",\n",
        "    apply_ocr=False\n",
        ")\n",
        "\n",
        "def preprocess(example):\n",
        "    image = example[\"image\"].convert(\"RGB\") # Convert image to RGB\n",
        "    encoding = processor(\n",
        "        image,\n",
        "        example[\"words\"],\n",
        "        boxes=example[\"bboxes\"],\n",
        "        word_labels=example[\"ner_tags\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\" # Ensure tensors are returned\n",
        "    )\n",
        "\n",
        "    # The processor returns tensors with a batch dimension of 1 for a single input.\n",
        "    # We need to remove this for each individual example before batching by Trainer's collator.\n",
        "    for k, v in encoding.items():\n",
        "        if isinstance(v, torch.Tensor) and v.ndim > 1 and v.shape[0] == 1:\n",
        "            encoding[k] = v.squeeze(0)\n",
        "\n",
        "    return encoding\n",
        "\n",
        "encoded_train = train_dataset.map(\n",
        "    preprocess,\n",
        "    batched=False,\n",
        "    remove_columns=train_dataset.column_names\n",
        ")\n",
        "\n",
        "encoded_test = test_dataset.map(\n",
        "    preprocess,\n",
        "    batched=False,\n",
        "    remove_columns=test_dataset.column_names\n",
        ")"
      ],
      "metadata": {
        "id": "MO1cc_B1AMYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LayoutLMv3ForTokenClassification\n",
        "from transformers import Trainer, TrainingArguments, DefaultDataCollator\n",
        "\n",
        "model = LayoutLMv3ForTokenClassification.from_pretrained(\n",
        "    \"microsoft/layoutlmv3-base\",\n",
        "    num_labels=len(LABELS),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./funsd_model\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    num_train_epochs=3,\n",
        "    eval_strategy=\"steps\",\n",
        "    logging_steps=100,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "data_collator = DefaultDataCollator()\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=encoded_train,\n",
        "    eval_dataset=encoded_test,\n",
        "    data_collator=data_collator # Use DefaultDataCollator for preprocessed data\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "zPgd6FNwAuPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def infer(example):\n",
        "    model.eval()\n",
        "\n",
        "    # Convert the image to RGB before processing, similar to the preprocess function\n",
        "    image_rgb = example[\"image\"].convert(\"RGB\")\n",
        "\n",
        "    encoding = processor(\n",
        "        image_rgb,\n",
        "        example[\"words\"],\n",
        "        boxes=example[\"bboxes\"],\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encoding)\n",
        "\n",
        "    predictions = outputs.logits.argmax(-1).squeeze().tolist()\n",
        "\n",
        "    results = []\n",
        "    for word, label_id in zip(example[\"words\"], predictions):\n",
        "        results.append({\n",
        "            \"word\": word,\n",
        "            \"label\": id2label[label_id]\n",
        "        })\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "MJFiBJ2BaaYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "def infer(example):\n",
        "    model.eval()\n",
        "\n",
        "    image = example[\"image\"]\n",
        "\n",
        "    # FIX: ensure image is RGB (3 channels)\n",
        "    if isinstance(image, Image.Image):\n",
        "        image = image.convert(\"RGB\")\n",
        "    elif isinstance(image, np.ndarray):\n",
        "        if image.ndim == 2:  # grayscale\n",
        "            image = np.stack([image]*3, axis=-1)\n",
        "        image = Image.fromarray(image).convert(\"RGB\")\n",
        "\n",
        "    encoding = processor(\n",
        "        image,\n",
        "        example[\"words\"],\n",
        "        boxes=example[\"bboxes\"],\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encoding)\n",
        "\n",
        "    predictions = outputs.logits.argmax(-1).squeeze().tolist()\n",
        "\n",
        "    results = []\n",
        "    for word, label_id in zip(example[\"words\"], predictions):\n",
        "        results.append((word, id2label[label_id]))\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# RUN THIS\n",
        "sample = test_dataset[0]\n",
        "predictions = infer(sample)\n",
        "\n",
        "for w, l in predictions:\n",
        "    print(w, \"->\", l)\n"
      ],
      "metadata": {
        "id": "D1Rlix-tfli_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}